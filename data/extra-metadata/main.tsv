paper_id	title	authors	abstract
2483-tacl	Planning, Inference, and Pragmatics in Sequential Language Games	Fereshte Khani, Noah D. Goodman and Percy Liang	We study sequential language games in which two players, each with private information, communicate to achieve a common goal. In such games, a successful player must (i) infer the partner's private information from the partner's messages, (ii) generate messages that are most likely to help with the goal, and (iii) reason pragmatically about the partner's strategy. We propose a model that captures all three characteristics and demonstrate their importance in capturing human behavior on a new goal-oriented dataset we collected using crowdsourcing.
2485-tacl	Grammar Error Correction in Morphologically-Rich Languages: The Case of Russian	Alla Rozovskaya and Dan Roth	Up until now, most of the research in grammar error correction focused on English, and the problem has hardly been explored for other languages. We address the task of correcting writing mistakes in morphologically-rich languages, with a focus on Russian. We present a corrected and error-tagged corpus of Russian learner writings and develop models that make use of existing state-of-the-art methods that have been well-studied for English. Although impressive results have recently been achieved for grammar error correction of non-native English writings, these results are limited to domains where plentiful training data is available. Since annotation is extremely costly, these approaches are not suitable for the majority of domains and languages. We thus focus on methods that use “minimal supervision”, i.e. those that do not rely on large amounts of annotated training data, and show how existing minimal- supervision approaches extend to a highly inflectional language such as Russian. The results demonstrate that these methods are particularly useful for correcting mistakes in grammatical phenomena that involve rich morphology.
2482-tacl	Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science	Emily M. Bender and Batya Friedman	In this paper, we propose _data statements_ as a design solution and professional practice for natural language processing technologists, in both research and development---through the adoption and widespread use of data statements, the field can begin to address critical scientific and ethical issues that result from the use of data from certain populations in the development of technology for other populations. We present a form that data statements can take and explore the implications of adopting them as part of regular practice. We argue that data statements will help alleviate issues related to exclusion and bias in language technology; lead to better precision in claims about how NLP research can generalize and thus better engineering results; protect companies from public embarrassment; and ultimately lead to language technology that meets its users in their own preferred linguistic style and furthermore does not misrepresent them to others.
2481-tacl	Attentive Convolution: Equipping CNNs with RNN-style Attention Mechanisms	Wenpeng Yin and Hinrich Schütze	In NLP, convolutional neural networks (CNNs) have benefited less than recurrent neural networks (RNNs) from attention mechanisms. We hypothesize that this is because the attention in CNNs has been mainly implemented as attentive pooling (i.e., it is applied to pooling) rather than as attentive convolution (i.e., it is integrated into convolution). Convolution is the differentiator of CNNs in that it can powerfully model the higher-level representation of a word by taking into account its local fixed-size context in the input text tx. In this work, we propose an attentive convolution network, ATTCONV. It extends the context scope of the convolution operation, deriving higher-level features for a word not only from local context, but also information extracted from nonlocal context by the attention mechanism commonly used in RNNs. This nonlocal context can come (i) from parts of the input text tx that are distant or (ii) from extra (i.e., external) contexts ty. Experiments on sentence modeling with zero-context (sentiment analysis), single-context (textual entailment) and multiple-context (claim verification) demonstrate the effectiveness of ATTCONV in sentence representation learning with the incorporation of context. In particular, attentive convolution outperforms attentive pooling and is a strong competitor to popular attentive RNNs.
2479-tacl	Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns	Kellie Webster, Marta Recasens, Vera Axelrod and Jason Baldridge	Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun-name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines which demonstrate the complexity of the challenge, the best achieving just 66.9% F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.
2486-tacl	Rotational Unit of Memory: A Novel Representation Unit for RNNs with Scalable Applications	Rumen Dangovski, Li Jing, Preslav Nakov, Mićo Tatalović and Marin Soljačić	Stacking long short-term memory (LSTM) cells or gated recurrent units (GRUs) as part of a recurrent neural network (RNN) has become a standard approach to solving a number of tasks ranging from language modeling to text summarization. While LSTMs and GRUs were designed to model long-range dependencies more accurately than conventional RNNs, they nevertheless have problems copying or recalling information from the long distant past. Here, we derive a phase-coded representation of the memory state, Rotational Unit of Memory (RUM), which unifies the concepts of unitary learning and associative memory. We show experimentally that RNNs based on RUMs can solve basic sequential tasks such as memory copying and memory recall much better than LSTMs/GRUs. We further demonstrate that by replacing LSTM/GRU with RUM units we can apply neural networks to real-world problems such as language modeling and text summarization, yielding results comparable to the state of the art.
2478-tacl	Analysis Methods in Neural Language Processing: A Survey	Yonatan Belinkov and James Glass	The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models has been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.
2484-tacl	DREAM: A Challenge Dataset and Models for Dialogue-Based Reading Comprehension	Kai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi and Claire Cardie	We present DREAM, the first dialogue-based multiple-choice reading comprehension dataset. Collected from English-as-a-foreign-language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our dataset contains 10,197 multiple-choice questions for 6,444 dialogues. In contrast to existing reading comprehension datasets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems: 84% of answers are non-extractive, 85% of questions require reasoning beyond a single sentence, and 34% of questions also involve commonsense knowledge. We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM dataset show the effectiveness of dialogue structure and general world knowledge. DREAM is available at https://dataset.org/dream/.
2480-tacl	CoQA: A Conversational Question Answering Challenge	Siva Reddy, Danqi Chen and Christopher D. Manning	Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets, e.g., coreference and pragmatic reasoning. We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4\%, which is 23.4~points behind human performance (88.8\%), indicating there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github.io/coqa/
53-srw	Kickstarting NLP for the Whole-person Function Domain with Representation Learning and Data Analysis	Denis Newman-Griffis	The growth of natural language processing into a variety of domains brings a variety of challenges. In this proposal, we analyze challenges in applying NLP techniques to information about whole-person function, a critical concept for resource management decisions in health systems. We describe initial characteristics of language focused on function, and identify three primary challenges of lack of vocabulary coverage, contextual sensitivity, and complex structure. We further show initial results demonstrating that representation learning techniques can mitigate some issues of contextual sensitivity in disambiguation. We propose as further work a combination of linguistic analysis and data-driven representation learning approaches to make progress on addressing these challenges and enabling the use of NLP tools for processing functioning information.
41-srw	Word Polysemy Aware Document Vector Estimation	Vivek Gupta, Ankit Saw, Harshit Gupta, Pegah Nokhiz and Partha Talukdar	Efficient representation of text documents is an important building block in many NLP tasks. Research on long text categorization has shown that simple weighted averaging of word vectors for sentence representation often outperforms more sophisticated neural models. Recently proposed Sparse Composite Document Vector (SCDV) extends this approach from sentences to documents using soft clustering over word vectors. However, SCDV disregards the multi-sense nature of words and it also suffers from the curse of higher dimensionality. In this work, we address these shortcomings and propose SCDV-MS. SCDV-MS utilizes multi-sense word embeddings and learns a lower dimensional manifold. Through extensive experiments on multiple real-world datasets, we show that SCDV-MS embeddings outperform previous state-of-the-art embeddings on multi-class and multi-label text categorization tasks. Furthermore, SCDV-MS embeddings are more efficient than SCDV in terms of time and space complexity in textual classification tasks. We have released SCDV-MS source code along with this paper.
47-srw	EQUATE : A Benchmark Evaluation Framework for Quantitative Reasoning in Natural Language Inference	Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose and Eduard Hovy	Quantitative reasoning is an important component of reasoning that any intelligent natural language understanding system can reasonably be expected to handle. We present EQUATE (Evaluating Quantitative Understanding Aptitude in Textual Entailment), a new dataset to evaluate the ability of models to reason with quantities in textual entailment (including not only arithmetic and algebraic computation, but also other phenomena such as range comparisons and verbal reasoning with quantities). The average performance of 9 published textual entailment models on EQUATE does not exceed a majority class baseline, indicating that current models do not implicitly learn to reason with quantities. We propose a new baseline Q-REAS that manipulates quantities symbolically, achieving some success on numerical reasoning, but struggling at more verbal aspects of the task. We hope our evaluation framework will support the development of new models of quantitative reasoning in language understanding.
60-srw	Learn Languages First and Then Convert: towards Effective Simplified to Traditional Chinese Conversion	Pranav A, S.F. Hui, I-Tsun Cheng, Ishaan Batra and Chiu Yik Hei	Simplified Chinese to Traditional Chinese conversion is a common preprocessing step in Chinese NLP. However, a simplified Chinese character could correspond to multiple traditional characters, and unfortunately, there is no accurate toolkit to disambiguate such mappings. We propose a sub-word segmentation model which relies on Simplified Chinese and Traditional Chinese language models and the character mapping table. Through these two language models, we effectively segment a sentence and use them to disambiguate between mappings. Our experiments show that we achieve the disambiguation accuracy of 98%.
