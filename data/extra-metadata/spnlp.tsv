paper_id	title	authors	abstract
3	Data selection and semi-supervised slot tagging for artificial intelligent agents	"Minmin Shen, James Zhu, Suranjit Adhikari and Angeliki Metallinou"	"We propose a semi-supervised learning framework to boost the performance of slot tagging in the low resource case, where we only have a small labeled target dataset available for model training, but we have access to a large unlabeled source dataset. Our framework consists of two components: first performing data selection to find a subset of the source data that is semantically similar to the target data, and second, training the model using both selected and target data through a combination of semi-supervised training techniques, inspired from self-training and self ensembling. We apply our techniques to a challenging slot tagging setup for a commercial artificial intelligent agent where unlabeled source data originates from largely different domains compared to the target data. We empirically show that our proposed techniques achieve up to 7% relative gain in low resource slot tagging compared to a strong pretrained model fine-tuned on target training data."
9	Supervised Contextual Embeddings for Transfer Learning in Natural Language Processing Tasks	"Mihir Kale, Aditya Siddhant, Sreyashi Nag, Radhika Parik, Anthony Tomasic and Matthais Grabmair"	"Pre-trained word embeddings are the primary method for transfer learning in several Natural Language Processing (NLP) tasks. Recent works have focused on using unsupervised techniques such as language modeling to obtain these embeddings. In contrast, this work focuses on extracting representations from multiple pre-trained supervised models, which enriches word embeddings with task and domain specific knowledge. Experiments performed in cross-task, cross-domain and crosslingual settings indicate that such supervised embeddings are helpful, especially in the lowresource setting, but the extent of gains is dependent on the nature of the task and domain."
15	Subword Language Model for Query Auto-Completion	Gyuwan Kim	"Current neural query auto-completion (QAC) systems rely on character-level language models. We present how to utilize subword language models for the fast and accurate generation of query completion candidates. Representing queries with subwords shorten a decoding length significantly. We develop a retrace algorithm and a reranking method by approximate marginalization to improve the robustness of the model. As a result, our model achieves up to 2.5 times faster while maintaining similar generation quality of generated results compared to the character-level baseline. Also, we propose a new evaluation metric, mean recoverable length (MRL), measuring how many upcoming characters the model could complete correctly. It provides more explicit meaning and eliminates the need for prefix length sampling for existing rank based metrics. Moreover, we performed a comprehensive analysis with ablation study to figure out the importance of each component."
